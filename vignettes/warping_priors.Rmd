---
title: "Adding priors to piecewise linear warping functions"
subtitle: "Binary and Gaussian cases"
author: "Erin McDonnell"
date: "3/24/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Binary case

For the binary registration case, the density is:
$$
P \Big( Y_i(t) | \mu_i(t) \Big) = \mu_i(t)^{Y_i(t)} \bigg(1 - \mu_i(t) \bigg)^{\Big(1 - Y_i(t) \Big)}
$$

The likelihood is:
$$
L \Big(Y_i | \mu_i, h_i^{-1} \Big) = \prod_{j = 1}^{D_i} \mu_i \Big(h^{-1}_i(t_{ij}^*) \Big)^{Y_i(t_{ij}^*)} \bigg( 1 - \mu_i\Big(h^{-1}_i(t_{ij}^*) \Big) \bigg)^{\Big(1 - Y_i(t_{ij}^*) \Big)}
$$

We can multiply the likelihood and the knot priors to get:
$$
L \Big( h_i^{-1} | Y_i, \mu_i \Big) = \Bigg\{ \prod_{j = 1}^{D_i} \mu_i \Big(h_{i}^{-1}(t_{ij}^*) \Big)^{Y_i(t_{ij}^*)} \bigg( 1 - \mu_i\Big(h_{i}^{-1}(t_{ij}^*) \Big) \bigg)^{\Big(1 - Y_i(t_{ij}^*) \Big)} \Bigg\} p(k_{1x}) p(k_{1y}) p(k_{2x}) p(k_{2y}).
$$

We want to minimize the negative log-likelihood:
$$
-\ell \Big( h_i^{-1} | Y_i, \mu_i \Big) = - \Bigg\{ \sum_{j = 1}^{D_i} Y_i(t_{ij}^*) \log \bigg(\mu_i \Big(h_{i}^{-1}(t_{ij}^*) \Big)\bigg) + \Big(1 - Y_i(t_{ij}^*) \Big) \log \bigg( 1 - \mu_i\Big(h_{i}^{-1}(t_{ij}^*) \Big) \bigg) \Bigg\} - \log \Big(p(k_{1x})\Big) - \log \Big(p(k_{1y})\Big) - \log \Big(p(k_{2x})\Big) - \log \Big(p(k_{2y})\Big).
$$

## Gaussian case

For the gaussian registration case, the density is:
$$
P \Big( Y_i(t) | \mu_i(t) \Big) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \Bigg\{ \frac{- \Big( Y_i(t) - \mu_i(t) \Big)^2}{2 \sigma^2} \Bigg\}.
$$

The likelihood is:
$$
P \Big( Y_i(t) | \mu_i(t) \Big) = (2 \pi \sigma^2)^\frac{-D_i}{2} \exp \Bigg\{ \frac{- \sum_{j = 1}^{D_i} \bigg( Y_i(t_{ij}^*) - \mu_i \Big(h_{i}^{-1} (t_{ij}^*) \Big)  \bigg)^2}{2 \sigma^2} \Bigg\}.
$$

We can multiply the likelihood and the knot priors to get:
$$
L \Big( h_i^{-1} | Y_i, \mu_i \Big) = (2 \pi \sigma^2)^\frac{-D_i}{2} \exp \Bigg\{ \frac{- \sum_{j = 1}^{D_i} \bigg( Y_i(t_{ij}^*) - \mu_i \Big(h_{i}^{-1} (t_{ij}^*) \Big)  \bigg)^2}{2 \sigma^2} \Bigg\} p(k_{1x}) p(k_{1y}) p(k_{2x}) p(k_{2y}).
$$

We want to minimize the negative log-likelihood:
$$
-\ell \Big( h_i^{-1} | Y_i, \mu_i \Big) = \frac{D_i}{2} \log (2 \pi \sigma^2) + \frac{1}{2\sigma^2} \sum_{j = 1}^{D_i} \bigg( Y_i(t_{ij}^*) - \mu_i \Big(h_{i}^{-1} (t_{ij}^*) \Big)  \bigg)^2 - \log \Big( p(k_{1x}) \Big) - \log \Big( p(k_{1y}) \Big) - \log \Big( p(k_{2x}) \Big) - \log \Big( p(k_{2y}) \Big).
$$